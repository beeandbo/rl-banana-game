import torch
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

from deep_q_model import DeepQModel
import random
from collections import deque
import random

BUFFER_SIZE = int(1e5)  # replay buffer size
BATCH_SIZE = 64         # minibatch size
GAMMA = 0.99            # discount factor
TAU = 1e-3              # for soft update of target parameters
LR = 5e-5               # learning rate
UPDATE_EVERY = 4        # how often to update the network
EPSILON_START = 1       # Epsilon params used in eps-greedy policy
EPSILON_DECAY = 0.995
EPSILON_MIN = 0.1

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

class Agent():
    """Deep Q RL Agent.

    This agent implements a version of the Deep Q Network from
    https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf.

    Unlike the network in the paper, this agent uses a simple fully connected
    network, and is meant to operate on simple inputs, not raw pixels.

    To use the agent, you should get action predictions from act() and
    then learn from the results using learn().
    """
    def __init__(self, state_size, action_size, seed, epsilon_start=EPSILON_START):
        """Initialize Agent.

        Params
        ======
            state_size (integer): Size of input state vector.
            action_size (integer): Number of possible actions.
            seed (float): Used to initialize random generator.  Use a consistent
                number to regularize behavior.
            start_epsilon (float): Controls starting epsilon used in
                Epsilon-greedy policy (see act() for more details).
        """
        self.state_size = state_size
        self.action_size = action_size
        self.seed = random.seed(seed)

        # Q-Network
        self.local_model = DeepQModel(state_size, action_size, seed).to(device)
        self.target_model = DeepQModel(state_size, action_size, seed).to(device)
        self.optimizer = optim.Adam(self.local_model.parameters(), lr=LR)

        # Replay memory
        self.memory = deque(maxlen=BUFFER_SIZE)
        self.eps = epsilon_start
        self.steps = 0
        self.last_action = None
        self.training = True

    def act(self, state, explore = True):
        """Returns actions for given state as per current policy.

        If expore is set to true, the returned state will follow an epsilon
        greedy policy, which means it returns the action predicted by the neural
        net with (1-get_epsilon()) probability, and otherwise returns a random
        action.


        Params
        ======
            state (array_like): current state
            explore (boolean): If true, explores a random action with
                self.get_epsilon() probability
        """
        self.steps += 1

        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        self.local_model.eval()
        with torch.no_grad():
            action_values = self.local_model.forward(state)
        self.local_model.train()
        # Epsilon-greedy action selection
        if not explore or random.random() > self.get_epsilon():
            action = np.argmax(action_values.cpu().data.numpy())
        else:
            action = random.choice(np.arange(self.action_size))
        return action

    def vectorize_experiences(self, experiences):
        """Vectorizes experience objects for use by pytorch

        Params
        ======
            experiences (array_like of Experience objects): Experiences to
                vectorize
        """
        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)
        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)
        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)
        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)
        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)

        return (states, actions, rewards, next_states, dones)

    def save(self, filename):
        """Saves learned params of underlying model to a checkpoint file.

        Params
        ======
            filename (string): Target file for saved data
        """
        torch.save(self.local_model.state_dict(), filename)

    def load(self, filename):
        """Loads learned params generated by save() into underlying model.

            filename (string): Path to file
        """
        self.local_model.load_state_dict(torch.load(filename))

        # Sync target with local
        self.target_model.load_state_dict(torch.load(filename))

    def train_model(self, experiences):
        """Trains model from an array of experiences.

        Params
        ======
            experiences (array_like of Experience objects): An array of
                experiences.
        """
        states, actions, rewards, next_states, dones = self.vectorize_experiences(experiences)

        Q_targets_next = self.target_model(next_states).detach().max(1)[0].unsqueeze(1)
        # Compute Q targets for current states
        Q_targets = rewards + (GAMMA * Q_targets_next * (1 - dones))

        # Get expected Q values from local model
        Q_expected = self.local_model(states).gather(1, actions)

        # Compute loss
        loss = F.mse_loss(Q_expected, Q_targets)

        # Minimize the loss
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # ------------------- update target network ------------------- #
        for target_param, local_param in zip(self.target_model.parameters(), self.local_model.parameters()):
            target_param.data.copy_(TAU*local_param.data + (1.0-TAU)*target_param.data)

    def reset_episode(self):
        "Marks the end of an episode."
        self.eps = max(EPSILON_MIN, self.eps * EPSILON_DECAY)

    def get_epsilon(self):
        "Return the current value of epsilon"
        return(self.eps)

    def learn(self, experience):
        """Update agent with the results of an experience.

        Params
        ======
            experience (Experience): Results of taking an action in the environment
        """

        self.memory.append(experience)
        if self.steps % UPDATE_EVERY == 0 and len(self.memory) >= BATCH_SIZE:
            self.train_model(random.sample(self.memory, k=BATCH_SIZE))

        if experience.done:
            self.reset_episode()
